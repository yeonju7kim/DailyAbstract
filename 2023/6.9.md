## OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework


## ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing
- zerocap: large-scale pretrained model의 knowledge를 이용하여 모든 단어를 search
  - 효율적, 다양성과 속도 측면에서 제한, controllability issue
- ConZIC: 새로운 sampling based non-autoregressive language model(Gibbs-BERT)
  - generate and continuously polish every word
  - Gibbs-BERT를 제안
    - flexible generation order
    - self-correct capapbility by bidirectional attention with faster and more diverse generation
    - Gibbs sampling 과 MLM의 결합, full context에서 polish the word
    - free of parameter updates
  - ConZIC는 flexible searching, higher diversity로 sentence를 generate
  - first controlable zero-shot IC method
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/5d7ad322-8642-4413-9dfa-fd29e8be85ba)
- image-matching score, fluent score, controllable score로 단어를 정함
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/1e669b35-052e-43d3-a790-e2e76705bdc0)

## nocaps: novel object captioning at scale
- 166100 사람의 annotation, 15100개의 이미지를 설명
- Open Images에 있는 이미지, COCO(80 classes), Open Images(600 classes)

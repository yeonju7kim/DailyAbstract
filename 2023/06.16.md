# FUSECAP: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions
https://arxiv.org/pdf/2305.17718v1.pdf
- 가끔 semantically significant elements를 capture을 fail
- general depiction을 제공하기 때문에 salient detail은 omit한다
- FUSECAP: additional visual information과 vision experts들을 섞어서 한다. object detectors, attribute recognizer, OCR,LLM
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/7e2f3107-209e-43bb-97d3-7116bf7dc886)

# Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models
- distribution shift가 있을 때 test-time adaptation(TTA)을 해야한다. 
- TTA with feedback 으로 overfitting을 피하고, model을 task goal로 align 하는 것
- CLIP을 reward model로 사용하여 feedback을 준다. CLIP reward를 maximize하도록 학습된다. reinforcement learning으로

# Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards
- Foundation model은 pretrained on vast unsupervised dataset, fine-tuned on labeled data
- Reinforcement learning은 network를 확장할 수 있다.
- 하지만 proxy reward의 imperfections은 training을 방해하고, suboptimal results를 이끈다.
- 이 논문에서는 다양한 rewards를 사용해서 multi-policy strategy 사용

# Contextual Object Detection with Multimodal Large Language Models
- MLLMs는 vision language task에서 잘 하지만 od에서 잘 못함
- contextual object detection
  - understanding visible objects within different human-AI interactive contexts
- representative scnarios가 조사됨
  - language cloze text
  - visual captioning
  - question answering
- ContextDET는 3가지 submodule
  - visual encoder 
  - pretrained LLM
  - visual decoder
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/6982402a-c234-4f2a-9606-c7ed09c444e4)
> OD를 VL task처럼 하겠다.

# Cross-Domain Image Captioning with Discriminative Finetuning
- 사람의 reference sentence를 mimic하려고 해서 vague captions을 만든다
- finetuning out-of-the-box neural captioner를 self-supervised discriminative commnuication objective로 학습을 해서 더 informative한 sentence를 생성
- 

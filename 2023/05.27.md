https://arxiv.org/pdf/2305.06500.pdf
## InstructBLIP:Towards General-purpose Vision-Language Models with Instruction Tuning
- universal한 모델이 여러 down-stream task에 적용되는 방법은 pre-training 과
- vision-language instruction tuning based on the pretrained BLIP-2
  - 26개의 dataset을 모아서 instruction tuning format으로 바꾸고, 두개의 클러스터(held-in instruction tuning, held-out zero-shot evaluation)로 분류
- instruction-aware visual feature extraction
  - 주어진 instruction으로 조정된 informative feature를 추출하는 방법

![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/57aaaece-1aa7-4867-9952-bc0afb5b829c)

---

https://arxiv.org/pdf/2305.05665.pdf
## ImageBind: One Embedding Space To Bind Them All
- joint embedding across six different modalities: images, text, audio, depth, thermal, IMU data (image-paired data로 충분하다)
- Image binding
  - recent large scale vision-language models도 이용
  - zero-shot capabilities to new modalities
- emergent applications 'out-of-the-box'
> 살짝 이해 안됨
> 
  - cross-modal retrieval
  - composing modalities with arithmetic
  - cross-modal detection and generation  
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/64cdc5a0-c0b9-46b6-95a5-ac557ec9b269)

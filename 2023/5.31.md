## DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification
https://arxiv.org/abs/2305.15957.pdf
- Motivation: 3D point cloud processing task에서 domain gap 때문에 limit이 생긴다
  - depth maps from 3D projection과 training images in CLIP
- 1. stable diffusion
  - ControlNet과 stable diffusion을 결합해서 domain gap을 최소화 하는 것 
- 2. style-prompt generation module
  - textual branch에서 few-shot tasks로서 소개가 되었다.
> stable diffusion을 사용해서 domain gap을 최소화해서 3D understanding을 가능하게 한건가?
> 왜 최소화 되지?
> 
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/4fc0b3f5-3bc0-43f9-b58a-e8f15d43050e)
- visual branch
  - multi-view realistic projection module
    - produce multi-view depth maps
  - stable-diffusion-based style transfer module
    - ControlNet + (Frozen) stable diffusion
    - transfer styles on the depth images
- textual branch
  - Style-Prompt Generation module
    - few-shot tasks + manual prompts for zero-shot tasks
- Multi-Modal Fusion Block
> 3D정보로 이미지를 만들어서 classification 하는게 왜 필요하지? 이 task를 잘 몰라서 이해가 안되네..

---
## HAAV:Hierarchical Aggregation of Augmented Views for Image Captioning
https://arxiv.org/pdf/2305.16295.pdf
- 어떻게 heterogeneouos set of encoding을 efficiently, effectively leveraging할 수 있는지?
- encoding을 augmented views of input image로 본다
- image captioning model은 shared encoder로 각각의 view를 독립적이고, 효과적으로 encode
- contrastive loss는 representation quality와 data efficiency를 향상하는 새로운?? 
> contrastiuve loss is incorporated across the encoded views in a novel way to improve their representation quality and the model's data efficiency
- hierarchical decoder를 사용해서 caption을 generate 할 때 도움되는 정도로 encoded view를 weigh
> 결국 different pre-training model로 여러 view를 나타낸다. 그것을 weigh 해서 captioning 한다.

---
## CRIS:CLIP-Driven Referring Image Segmentation
https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.pdf
- 기존에는 독립적으로 학습된 language/vision pretrained model의 knowledge를 이용했다.
- end-to-end CLIP-Driven Referring Image Segmentation framework(CRIS) 제안
- multi-modal knowledge를 잘 transfer, text-to-pixel alignment를 달성하기 위해 
  - 1) vision-language decoding, 2) contrastive learning을 사용
- 1. vision-language decoder
  - fine-grained semantic information을 textual representation에서 pixel-level activation으로 propagation한다
  - 두 modality간의 consistency를 promotes
- 2. text-to-pixel contrastive learning 
  - text featrue가 관련 pixel-level feature 비슷해지도록
> pixel 단위로 

## OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework


## ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing
- zerocap: large-scale pretrained model의 knowledge를 이용하여 모든 단어를 search
  - 효율적, 다양성과 속도 측면에서 제한, controllability issue
- ConZIC: 새로운 sampling based non-autoregressive language model(Gibbs-BERT)
  - generate and continuously polish every word
  - Gibbs-BERT를 제안
    - flexible generation order
    - self-correct capapbility by bidirectional attention with faster and more diverse generation
    - Gibbs sampling 과 MLM의 결합, full context에서 polish the word
    - free of parameter updates
  - ConZIC는 flexible searching, higher diversity로 sentence를 generate
  - first controlable zero-shot IC method
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/5d7ad322-8642-4413-9dfa-fd29e8be85ba)
- image-matching score, fluent score, controllable score로 단어를 정함
![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/1e669b35-052e-43d3-a790-e2e76705bdc0)

|Title|Description|
|---|---|
|nocaps: novel object captioning at scale|166100 사람의 annotation, 15100개의 이미지를 설명<br>Open Images에 있는 이미지, COCO(80 classes), Open Images(600 classes), 400개 정도는 training data와 association 적다|
|VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning|VIVO는 image-tag data를 이용하여 visual vocabulary를 학습<br>image-level tags와 image region feature를 align, hungarian matching 사용|
|Pointing Novel Objects in Image Captioning||

CLIPScore: A Reference-free Evaluation Metric for Image Captioning

![image](https://github.com/yeonju7kim/DailyAbstract/assets/95571735/78537d70-8f94-4502-91ff-6aa3c494d8e6)

## 


## 
